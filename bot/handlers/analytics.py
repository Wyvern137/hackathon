"""
–û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏
"""
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, Any, List, Optional
from telegram import Update
from telegram.ext import ContextTypes
from bot.utils.helpers import get_or_create_user
from bot.database.models import ContentHistory, ContentPlan, PostTemplate
from bot.database.database import get_db
from bot.services.analytics.predictions import prediction_service
from telegram import InlineKeyboardMarkup, InlineKeyboardButton

try:
    import matplotlib
    matplotlib.use('Agg')  # –ò—Å–ø–æ–ª—å–∑—É–µ–º backend –±–µ–∑ GUI
    import matplotlib.pyplot as plt
    import matplotlib.dates as mdates
    matplotlib_available = True
except ImportError:
    matplotlib_available = False
    logging.warning("matplotlib –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –±—É–¥–µ—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")

logger = logging.getLogger(__name__)

CHARTS_DIR = Path("data/charts")
CHARTS_DIR.mkdir(parents=True, exist_ok=True)


async def show_statistics(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –±–æ—Ç–∞"""
    user_id = update.effective_user.id
    get_or_create_user(user_id, update.effective_user.username, update.effective_user.first_name or "")
    
    with get_db() as db:
        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_texts = db.query(ContentHistory).filter(
            ContentHistory.user_id == user_id,
            ContentHistory.content_type == "text"
        ).count()
        
        total_images = db.query(ContentHistory).filter(
            ContentHistory.user_id == user_id,
            ContentHistory.content_type == "image"
        ).count()
        
        total_plans = db.query(ContentHistory).filter(
            ContentHistory.user_id == user_id,
            ContentHistory.content_type == "plan"
        ).count()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 7 –¥–Ω–µ–π
        week_ago = datetime.now() - timedelta(days=7)
        texts_week = db.query(ContentHistory).filter(
            ContentHistory.user_id == user_id,
            ContentHistory.content_type == "text",
            ContentHistory.generated_at >= week_ago
        ).count()
        
        images_week = db.query(ContentHistory).filter(
            ContentHistory.user_id == user_id,
            ContentHistory.content_type == "image",
            ContentHistory.generated_at >= week_ago
        ).count()
        
        # –ê–∫—Ç–∏–≤–Ω—ã–µ –ø–ª–∞–Ω—ã
        active_plans = db.query(ContentPlan).filter(
            ContentPlan.user_id == user_id,
            ContentPlan.is_active == True
        ).count()
        
        # –®–∞–±–ª–æ–Ω—ã
        templates_count = db.query(PostTemplate).filter(
            PostTemplate.user_id == user_id
        ).count()
        
        # –°–∞–º–∞—è –ø–æ–ø—É–ª—è—Ä–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
        all_items = db.query(ContentHistory).filter(
            ContentHistory.user_id == user_id
        ).all()
        
        function_usage = {
            "text": 0,
            "image": 0,
            "plan": 0
        }
        
        for item in all_items:
            if item.content_type in function_usage:
                function_usage[item.content_type] += 1
        
        most_popular = max(function_usage.items(), key=lambda x: x[1])[0] if function_usage else "text"
        most_popular_names = {
            "text": "üìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞",
            "image": "üé® –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π",
            "plan": "üìÖ –ö–æ–Ω—Ç–µ–Ω—Ç-–ø–ª–∞–Ω"
        }
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å—Ç–∏–ª—è–º (–¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏)
        text_types = db.query(ContentHistory).filter(
            ContentHistory.user_id == user_id,
            ContentHistory.content_type == "text"
        ).all()
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ
    text = (
        "üìä **–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è**\n\n"
        "**–û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:**\n"
        f"üìù –¢–µ–∫—Å—Ç–æ–≤ —Å–æ–∑–¥–∞–Ω–æ: {total_texts}\n"
        f"üé® –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å–æ–∑–¥–∞–Ω–æ: {total_images}\n"
        f"üìÖ –ö–æ–Ω—Ç–µ–Ω—Ç-–ø–ª–∞–Ω–æ–≤: {total_plans}\n"
        f"üìã –®–∞–±–ª–æ–Ω–æ–≤: {templates_count}\n\n"
        
        "**–ó–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 7 –¥–Ω–µ–π:**\n"
        f"üìù –¢–µ–∫—Å—Ç–æ–≤: {texts_week}\n"
        f"üé® –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {images_week}\n\n"
        
        "**–ê–∫—Ç–∏–≤–Ω—ã–µ –ø–ª–∞–Ω—ã:**\n"
        f"üìÖ –ê–∫—Ç–∏–≤–Ω—ã—Ö –∫–æ–Ω—Ç–µ–Ω—Ç-–ø–ª–∞–Ω–æ–≤: {active_plans}\n\n"
        
        "**–°–∞–º–∞—è –ø–æ–ø—É–ª—è—Ä–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è:**\n"
        f"‚≠ê {most_popular_names.get(most_popular, '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö')}\n\n"
    )
    
    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    if total_texts > 0:
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å—Ç–∏–ª—è–º
        style_stats = {}
        for item in text_types:
            if item.content_type == "text":
                content_data = item.content_data if isinstance(item.content_data, dict) else {}
                style = content_data.get("style", "–Ω–µ —É–∫–∞–∑–∞–Ω")
                style_stats[style] = style_stats.get(style, 0) + 1
        
        if style_stats:
            text += "**–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å—Ç–∏–ª—è–º:**\n"
            for style, count in sorted(style_stats.items(), key=lambda x: x[1], reverse=True):
                text += f"‚Ä¢ {style}: {count}\n"
            text += "\n"
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∑–∞ –º–µ—Å—è—Ü
        month_ago = datetime.now() - timedelta(days=30)
        with get_db() as db_month:
            texts_month = db_month.query(ContentHistory).filter(
                ContentHistory.user_id == user_id,
                ContentHistory.content_type == "text",
                ContentHistory.generated_at >= month_ago
            ).count()
        
        text += "**–ó–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π –º–µ—Å—è—Ü:**\n"
        text += f"üìù –¢–µ–∫—Å—Ç–æ–≤: {texts_month}\n\n"
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    if total_texts == 0:
        text += "üí° –°–æ–≤–µ—Ç: –ü–æ–ø—Ä–æ–±—É–π —Å–æ–∑–¥–∞—Ç—å —Å–≤–æ–π –ø–µ—Ä–≤—ã–π –ø–æ—Å—Ç!"
    elif templates_count == 0 and total_texts > 3:
        text += "üí° –°–æ–≤–µ—Ç: –°–æ–∑–¥–∞–π —à–∞–±–ª–æ–Ω –∏–∑ —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –ø–æ—Å—Ç–æ–≤ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã!"
    elif active_plans == 0:
        text += "üí° –°–æ–≤–µ—Ç: –°–æ–∑–¥–∞–π –∫–æ–Ω—Ç–µ–Ω—Ç-–ø–ª–∞–Ω –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–Ω—ã—Ö –ø—É–±–ª–∏–∫–∞—Ü–∏–π!"
    elif texts_month == 0 and total_texts > 0:
        text += "üí° –°–æ–≤–µ—Ç: –ó–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π –º–µ—Å—è—Ü –Ω–µ—Ç –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏. –í—Ä–µ–º—è –≤–µ—Ä–Ω—É—Ç—å—Å—è –∫ —Å–æ–∑–¥–∞–Ω–∏—é –∫–æ–Ω—Ç–µ–Ω—Ç–∞!"
    
    # –ö–Ω–æ–ø–∫–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏
    analytics_keyboard = InlineKeyboardMarkup([
        [
            InlineKeyboardButton("üìà –ì—Ä–∞—Ñ–∏–∫ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏", callback_data="analytics_chart"),
            InlineKeyboardButton("üìä –ê–Ω–∞–ª–∏–∑ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏", callback_data="analytics_popularity")
        ],
        [
            InlineKeyboardButton("üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏", callback_data="analytics_recommendations"),
            InlineKeyboardButton("üîÆ –ü—Ä–æ–≥–Ω–æ–∑ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏", callback_data="analytics_predictions")
        ],
        [
            InlineKeyboardButton("‚è∞ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≤—Ä–µ–º–µ–Ω–∏", callback_data="analytics_posting_time")
        ]
    ])
    
    await update.message.reply_text(text, reply_markup=analytics_keyboard, parse_mode="Markdown")


def get_detailed_statistics(user_id: int, period_days: int = 30) -> Dict[str, Any]:
    """
    –ü–æ–ª—É—á–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∑–∞ –ø–µ—Ä–∏–æ–¥
    
    Args:
        user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        period_days: –ü–µ—Ä–∏–æ–¥ –≤ –¥–Ω—è—Ö
    
    Returns:
        Dict —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
    """
    try:
        start_date = datetime.now() - timedelta(days=period_days)
        
        with get_db() as db:
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            texts = db.query(ContentHistory).filter(
                ContentHistory.user_id == user_id,
                ContentHistory.content_type == "text",
                ContentHistory.generated_at >= start_date
            ).all()
            
            images = db.query(ContentHistory).filter(
                ContentHistory.user_id == user_id,
                ContentHistory.content_type == "image",
                ContentHistory.generated_at >= start_date
            ).all()
            
            plans = db.query(ContentHistory).filter(
                ContentHistory.user_id == user_id,
                ContentHistory.content_type == "plan",
                ContentHistory.generated_at >= start_date
            ).all()
            
            # –ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ –¥–Ω—è–º
            daily_activity = {}
            for item in texts + images + plans:
                day = item.generated_at.date()
                daily_activity[day] = daily_activity.get(day, 0) + 1
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å—Ç–∏–ª—è–º
            style_stats = {}
            for item in texts:
                content_data = item.content_data if isinstance(item.content_data, dict) else {}
                style = content_data.get("style", "–Ω–µ —É–∫–∞–∑–∞–Ω")
                style_stats[style] = style_stats.get(style, 0) + 1
            
            # –°–∞–º–∞—è –∞–∫—Ç–∏–≤–Ω–∞—è –Ω–µ–¥–µ–ª—è
            weekly_activity = {}
            for day, count in daily_activity.items():
                week_start = day - timedelta(days=day.weekday())
                weekly_activity[week_start] = weekly_activity.get(week_start, 0) + count
            
            most_active_week = max(weekly_activity.items(), key=lambda x: x[1]) if weekly_activity else None
            
            return {
                "success": True,
                "period_days": period_days,
                "texts_count": len(texts),
                "images_count": len(images),
                "plans_count": len(plans),
                "total_count": len(texts) + len(images) + len(plans),
                "daily_activity": daily_activity,
                "style_stats": style_stats,
                "most_active_week": most_active_week[0].isoformat() if most_active_week else None,
                "most_active_week_count": most_active_week[1] if most_active_week else 0
            }
    
    except Exception as e:
        logger.exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
        return {"success": False, "error": str(e)}


async def analyze_content_popularity(user_id: int, content_type: str = "text", limit: int = 10) -> Dict[str, Any]:
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –º–µ—Ç—Ä–∏–∫–∞–º
    
    Args:
        user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        content_type: –¢–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (text, image, plan)
        limit: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    
    Returns:
        Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞
    """
    try:
        with get_db() as db:
            items = db.query(ContentHistory).filter(
                ContentHistory.user_id == user_id,
                ContentHistory.content_type == content_type
            ).order_by(ContentHistory.generated_at.desc()).limit(limit).all()
        
        if not items:
            return {"success": False, "error": "–ö–æ–Ω—Ç–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω"}
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç—ã
        if content_type == "text":
            popularity_metrics = {
                "most_used_hashtags": {},
                "most_common_styles": {},
                "average_length": 0,
                "total_hashtags": 0
            }
            
            total_length = 0
            for item in items:
                content_data = item.content_data if isinstance(item.content_data, dict) else {}
                text = content_data.get("text", str(content_data))
                
                # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –¥–ª–∏–Ω—É
                total_length += len(text)
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ö–µ—à—Ç–µ–≥–∏
                hashtags = content_data.get("hashtags", [])
                for tag in hashtags:
                    popularity_metrics["most_used_hashtags"][tag] = popularity_metrics["most_used_hashtags"].get(tag, 0) + 1
                    popularity_metrics["total_hashtags"] += 1
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç–∏–ª–∏
                style = content_data.get("style", "–Ω–µ —É–∫–∞–∑–∞–Ω")
                popularity_metrics["most_common_styles"][style] = popularity_metrics["most_common_styles"].get(style, 0) + 1
            
            popularity_metrics["average_length"] = round(total_length / len(items), 1) if items else 0
            popularity_metrics["most_used_hashtags"] = dict(sorted(
                popularity_metrics["most_used_hashtags"].items(),
                key=lambda x: x[1],
                reverse=True
            )[:10])
            popularity_metrics["most_common_styles"] = dict(sorted(
                popularity_metrics["most_common_styles"].items(),
                key=lambda x: x[1],
                reverse=True
            ))
            
            return {
                "success": True,
                "content_type": content_type,
                "items_analyzed": len(items),
                "metrics": popularity_metrics
            }
        
        return {"success": False, "error": "–¢–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏"}
    
    except Exception as e:
        logger.exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏: {e}")
        return {"success": False, "error": str(e)}


async def generate_activity_chart(user_id: int, period_days: int = 30) -> Optional[Path]:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≥—Ä–∞—Ñ–∏–∫ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    
    Args:
        user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        period_days: –ü–µ—Ä–∏–æ–¥ –≤ –¥–Ω—è—Ö
    
    Returns:
        Path –∫ —Ñ–∞–π–ª—É —Å –≥—Ä–∞—Ñ–∏–∫–æ–º –∏–ª–∏ None
    """
    if not matplotlib_available:
        logger.warning("matplotlib –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –≥—Ä–∞—Ñ–∏–∫ –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–æ–∑–¥–∞–Ω")
        return None
    
    try:
        stats = get_detailed_statistics(user_id, period_days)
        
        if not stats.get("success") or not stats.get("daily_activity"):
            return None
        
        daily_activity = stats["daily_activity"]
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–∞–º
        sorted_dates = sorted(daily_activity.keys())
        dates = sorted_dates
        counts = [daily_activity[date] for date in dates]
        
        # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–∏–∫
        plt.figure(figsize=(12, 6))
        plt.plot(dates, counts, marker='o', linestyle='-', linewidth=2, markersize=5)
        plt.fill_between(dates, counts, alpha=0.3)
        plt.title(f'–ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∑–∞ {period_days} –¥–Ω–µ–π', fontsize=14, fontweight='bold')
        plt.xlabel('–î–∞—Ç–∞', fontsize=12)
        plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Å—Ç–æ–≤', fontsize=12)
        plt.grid(True, alpha=0.3)
        plt.xticks(rotation=45)
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –¥–∞—Ç—ã
        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%d.%m'))
        plt.gcf().autofmt_xdate()
        
        plt.tight_layout()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä–∞—Ñ–∏–∫
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"activity_{user_id}_{timestamp}.png"
        file_path = CHARTS_DIR / filename
        plt.savefig(file_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        logger.info(f"–ì—Ä–∞—Ñ–∏–∫ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Å–æ–∑–¥–∞–Ω: {file_path}")
        return file_path
    
    except Exception as e:
        logger.exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –≥—Ä–∞—Ñ–∏–∫–∞ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏: {e}")
        return None


def generate_recommendations(user_id: int) -> List[str]:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    
    Args:
        user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    
    Returns:
        –°–ø–∏—Å–æ–∫ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
    """
    recommendations = []
    
    try:
        stats = get_detailed_statistics(user_id, period_days=30)
        
        if not stats.get("success"):
            return ["–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"]
        
        total_count = stats.get("total_count", 0)
        texts_count = stats.get("texts_count", 0)
        images_count = stats.get("images_count", 0)
        plans_count = stats.get("plans_count", 0)
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        if total_count == 0:
            recommendations.append("üí° –ù–∞—á–Ω–∏ —Å —Å–æ–∑–¥–∞–Ω–∏—è –ø–µ—Ä–≤–æ–≥–æ –ø–æ—Å—Ç–∞!")
        elif texts_count > images_count * 3:
            recommendations.append("üí° –ü–æ–ø—Ä–æ–±—É–π —Å–æ–∑–¥–∞—Ç—å –±–æ–ª—å—à–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è!")
        elif images_count > texts_count * 3:
            recommendations.append("üí° –î–æ–±–∞–≤—å –±–æ–ª—å—à–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø–æ—Å—Ç–æ–≤ –¥–ª—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞!")
        elif plans_count == 0:
            recommendations.append("üí° –°–æ–∑–¥–∞–π –∫–æ–Ω—Ç–µ–Ω—Ç-–ø–ª–∞–Ω –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–Ω—ã—Ö –ø—É–±–ª–∏–∫–∞—Ü–∏–π!")
        elif stats.get("most_active_week_count", 0) > 10:
            recommendations.append("‚≠ê –û—Ç–ª–∏—á–Ω–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å! –ü—Ä–æ–¥–æ–ª–∂–∞–π –≤ —Ç–æ–º –∂–µ –¥—É—Ö–µ!")
        
        # –ê–Ω–∞–ª–∏–∑ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        daily_activity = stats.get("daily_activity", {})
        if len(daily_activity) < 5:
            recommendations.append("üìÖ –ü–æ–ø—Ä–æ–±—É–π –ø—É–±–ª–∏–∫–æ–≤–∞—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç —Ä–µ–≥—É–ª—è—Ä–Ω–µ–µ –¥–ª—è –ª—É—á—à–µ–≥–æ –æ—Ö–≤–∞—Ç–∞!")
        elif len(daily_activity) > 20:
            recommendations.append("üî• –í—ã—Å–æ–∫–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å! –û—Ç–ª–∏—á–Ω–∞—è —Ä–∞–±–æ—Ç–∞!")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Å—Ç–∏–ª—è–º
        style_stats = stats.get("style_stats", {})
        if len(style_stats) == 1:
            recommendations.append("üí° –ü–æ–ø—Ä–æ–±—É–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å —Å —Ä–∞–∑–Ω—ã–º–∏ —Å—Ç–∏–ª—è–º–∏ –∏–∑–ª–æ–∂–µ–Ω–∏—è!")
        
        if not recommendations:
            recommendations.append("‚ú® –ü—Ä–æ–¥–æ–ª–∂–∞–π —Å–æ–∑–¥–∞–≤–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç!")
        
        return recommendations
    
    except Exception as e:
        logger.exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {e}")
        return ["–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏"]


async def handle_analytics_callback(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ callback –¥–ª—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∏"""
    query = update.callback_query
    await query.answer()
    
    user_id = update.effective_user.id
    callback_data = query.data
    
    if callback_data == "analytics_chart":
        await query.edit_message_text("‚è≥ –ì–µ–Ω–µ—Ä–∏—Ä—É—é –≥—Ä–∞—Ñ–∏–∫ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏...")
        
        chart_path = await generate_activity_chart(user_id, period_days=30)
        
        if chart_path and chart_path.exists():
            with open(chart_path, 'rb') as f:
                await query.message.reply_photo(
                    photo=f,
                    caption="üìà –ì—Ä–∞—Ñ–∏–∫ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –¥–Ω–µ–π"
                )
            await query.edit_message_text("‚úÖ –ì—Ä–∞—Ñ–∏–∫ —Å–æ–∑–¥–∞–Ω!")
        else:
            await query.edit_message_text(
                "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –≥—Ä–∞—Ñ–∏–∫. –í–æ–∑–º–æ–∂–Ω–æ, –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ matplotlib –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞."
            )
    
    elif callback_data == "analytics_popularity":
        await query.edit_message_text("‚è≥ –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç–∞...")
        
        analysis = await analyze_content_popularity(user_id, content_type="text", limit=50)
        
        if analysis.get("success"):
            metrics = analysis.get("metrics", {})
            text = "üìä **–ê–Ω–∞–ª–∏–∑ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞**\n\n"
            
            # –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ —Ö–µ—à—Ç–µ–≥–∏
            hashtags = metrics.get("most_used_hashtags", {})
            if hashtags:
                text += "**–¢–æ–ø —Ö–µ—à—Ç–µ–≥–æ–≤:**\n"
                for i, (tag, count) in enumerate(list(hashtags.items())[:10], 1):
                    text += f"{i}. #{tag}: {count} —Ä–∞–∑\n"
                text += "\n"
            
            # –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ —Å—Ç–∏–ª–∏
            styles = metrics.get("most_common_styles", {})
            if styles:
                text += "**–ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ —Å—Ç–∏–ª–∏:**\n"
                for style, count in sorted(styles.items(), key=lambda x: x[1], reverse=True):
                    style_names = {
                        "conversational": "–†–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–π",
                        "formal": "–û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π",
                        "artistic": "–•—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–π",
                        "neutral": "–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π",
                        "friendly": "–î—Ä—É–∂–µ–ª—é–±–Ω—ã–π"
                    }
                    style_name = style_names.get(style, style)
                    text += f"‚Ä¢ {style_name}: {count} —Ä–∞–∑\n"
                text += "\n"
            
            # –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞
            avg_length = metrics.get("average_length", 0)
            total_hashtags = metrics.get("total_hashtags", 0)
            items_count = analysis.get("items_analyzed", 0)
            
            text += f"**–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:**\n"
            text += f"‚Ä¢ –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –ø–æ—Å—Ç–æ–≤: {items_count}\n"
            text += f"‚Ä¢ –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {avg_length:.0f} —Å–∏–º–≤–æ–ª–æ–≤\n"
            text += f"‚Ä¢ –í—Å–µ–≥–æ —Ö–µ—à—Ç–µ–≥–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: {total_hashtags}\n"
            
            await query.edit_message_text(text, parse_mode="Markdown")
        else:
            await query.edit_message_text(
                f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ: {analysis.get('error', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')}"
            )
    
    elif callback_data == "analytics_recommendations":
        await query.edit_message_text("‚è≥ –ì–µ–Ω–µ—Ä–∏—Ä—É—é —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏...")
        
        recommendations = generate_recommendations(user_id)
        
        if recommendations:
            text = "üí° **–ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏**\n\n"
            for rec in recommendations:
                text += f"{rec}\n"
            
            await query.edit_message_text(text, parse_mode="Markdown")
        else:
            await query.edit_message_text("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏.")
    
    elif callback_data == "analytics_predictions":
        await query.edit_message_text("‚è≥ –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å...")
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø–æ—Å—Ç –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞
        with get_db() as db:
            last_post = db.query(ContentHistory).filter(
                ContentHistory.user_id == user_id,
                ContentHistory.content_type == "text"
            ).order_by(ContentHistory.generated_at.desc()).first()
        
        if last_post:
            content_data = last_post.content_data if isinstance(last_post.content_data, dict) else {}
            text = content_data.get("text", "")
            hashtags = content_data.get("hashtags", [])
            style = content_data.get("style", "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π")
            
            prediction = await prediction_service.predict_reach(user_id, text, hashtags, style)
            
            if prediction.get("success"):
                reach_emoji = {
                    "–≤—ã—Å–æ–∫–∏–π": "üî•",
                    "—Å—Ä–µ–¥–Ω–∏–π": "üìä",
                    "–Ω–∏–∑–∫–∏–π": "üìâ"
                }
                emoji = reach_emoji.get(prediction["predicted_reach"], "üìä")
                
                text_response = (
                    f"{emoji} **–ü—Ä–æ–≥–Ω–æ–∑ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏**\n\n"
                    f"**–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π –æ—Ö–≤–∞—Ç:** {prediction['predicted_reach']}\n"
                    f"**–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å:** {prediction['confidence']}\n\n"
                )
                
                if prediction.get("recommendations"):
                    text_response += f"**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**\n{prediction['recommendations']}\n\n"
                
                if prediction.get("metrics"):
                    metrics = prediction["metrics"]
                    text_response += (
                        f"**–ú–µ—Ç—Ä–∏–∫–∏:**\n"
                        f"‚Ä¢ –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {metrics['text_length']} —Å–∏–º–≤–æ–ª–æ–≤\n"
                        f"‚Ä¢ –•–µ—à—Ç–µ–≥–æ–≤: {metrics['hashtags_count']}\n"
                        f"‚Ä¢ –°—Ç–∏–ª—å: {metrics['style']}\n"
                    )
                
                await query.edit_message_text(text_response, parse_mode="Markdown")
            else:
                await query.edit_message_text(
                    f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–∏: {prediction.get('error', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')}"
                )
        else:
            await query.edit_message_text(
                "‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ—Å—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞. –°–æ–∑–¥–∞–π —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –ø–æ—Å—Ç!"
            )
    
    elif callback_data == "analytics_posting_time":
        await query.edit_message_text("‚è≥ –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –ª—É—á—à–µ–µ –≤—Ä–µ–º—è –¥–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–π...")
        
        recommendation = await prediction_service.recommend_posting_time(user_id)
        
        if recommendation.get("success"):
            times = recommendation.get("recommended_times", [])
            text = "‚è∞ **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–π**\n\n"
            text += "**–õ—É—á—à–µ–µ –≤—Ä–µ–º—è –¥–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–π:**\n"
            for i, time_str in enumerate(times, 1):
                text += f"{i}. {time_str}\n"
            
            if recommendation.get("analysis"):
                analysis = recommendation["analysis"]
                if analysis.get("most_active_hour") is not None:
                    text += f"\n**–ê–Ω–∞–ª–∏–∑:**\n"
                    text += f"‚Ä¢ –°–∞–º—ã–π –∞–∫—Ç–∏–≤–Ω—ã–π —á–∞—Å: {analysis['most_active_hour']}:00\n"
            
            text += "\nüí° *–°–æ–≤–µ—Ç:* –ü—É–±–ª–∏–∫—É–π –∫–æ–Ω—Ç–µ–Ω—Ç –≤ —ç—Ç–∏ —á–∞—Å—ã –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –æ—Ö–≤–∞—Ç–∞!"
            
            await query.edit_message_text(text, parse_mode="Markdown")
        else:
            await query.edit_message_text(
                f"‚ùå –û—à–∏–±–∫–∞: {recommendation.get('error', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')}"
            )


def setup_analytics_handlers(application):
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏"""
    from telegram.ext import CallbackQueryHandler
    application.add_handler(
        CallbackQueryHandler(handle_analytics_callback, pattern="^analytics_")
    )
    
    # –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ö–≤–∞—Ç–∞ –ø–æ—Å—Ç–∞
    async def handle_post_prediction(update: Update, context: ContextTypes.DEFAULT_TYPE):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –ø—Ä–æ–≥–Ω–æ–∑–∞ –¥–ª—è –≥–æ—Ç–æ–≤–æ–≥–æ –ø–æ—Å—Ç–∞"""
        query = update.callback_query
        if not query:
            return
        
        await query.answer()
        
        if query.data == "predict_reach":
            user_id = update.effective_user.id
            last_text = context.user_data.get('last_generated_text', '')
            last_data = context.user_data.get('last_text_data', {})
            
            if not last_text:
                await query.answer("–ü–æ—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω", show_alert=True)
                return
            
            await query.edit_message_text("‚è≥ –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É—é –æ—Ö–≤–∞—Ç –ø–æ—Å—Ç–∞...")
            
            text = last_data.get("text", last_text)
            hashtags = last_data.get("hashtags", [])
            style = context.user_data.get('style', '–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π')
            
            prediction = await prediction_service.predict_reach(user_id, text, hashtags, style)
            
            if prediction.get("success"):
                reach_emoji = {
                    "–≤—ã—Å–æ–∫–∏–π": "üî•",
                    "—Å—Ä–µ–¥–Ω–∏–π": "üìä",
                    "–Ω–∏–∑–∫–∏–π": "üìâ"
                }
                emoji = reach_emoji.get(prediction["predicted_reach"], "üìä")
                
                text_response = (
                    f"{emoji} **–ü—Ä–æ–≥–Ω–æ–∑ –æ—Ö–≤–∞—Ç–∞**\n\n"
                    f"**–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π –æ—Ö–≤–∞—Ç:** {prediction['predicted_reach']}\n"
                    f"**–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å:** {prediction['confidence']}\n"
                )
                
                if prediction.get("recommendations"):
                    text_response += f"\n**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**\n{prediction['recommendations']}"
                
                await query.edit_message_text(text_response, parse_mode="Markdown")
    
    application.add_handler(
        CallbackQueryHandler(handle_post_prediction, pattern="^predict_reach$")
    )

